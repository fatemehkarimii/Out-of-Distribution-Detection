{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnW1RLyFYzNE",
        "outputId": "b5224ac5-5230-4d22-935a-042874e9a553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQbkLcGEy9dk"
      },
      "outputs": [],
      "source": [
        "#@title REQ\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import pathlib\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dataset\n",
        "import torchvision.utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "# Difine Variables\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "IMAGE_CHANNEL = 3\n",
        "Z_DIM = 100                 # Random vector dimension\n",
        "G_HIDDEN = 64\n",
        "X_DIM = 64                  # The width/height of the generated images.\n",
        "D_HIDDEN = 64\n",
        "EPOCH_NUM = 50000\n",
        "REAL_LABEL = 1\n",
        "FAKE_LABEL = 0\n",
        "lr = 2e-4\n",
        "ngpu = 1                   # Number of GPUs available. Use 0 for CPU mode\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk8mIWhIfP88"
      },
      "outputs": [],
      "source": [
        "#@title Lateral Data\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class OODImageDataset(Dataset):\n",
        "    def __init__(self, root, transform=None, target_transform=None, target_class='LATERAL'):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.samples = []  # List to store sample paths and labels\n",
        "\n",
        "\n",
        "\n",
        "        # Ensure that the target_class is valid\n",
        "        if target_class not in ['LATERAL']:\n",
        "            raise ValueError(\"target_class must be 'LATERAL'.\")\n",
        "\n",
        "        for class_name in os.listdir(root):\n",
        "            if class_name == target_class:\n",
        "                class_dir = os.path.join(root, class_name)\n",
        "                if os.path.isdir(class_dir):\n",
        "                    class_label = class_name\n",
        "                    for img_name in os.listdir(class_dir):\n",
        "                        img_path = os.path.join(class_dir, img_name)\n",
        "                        if not img_name.startswith('.DS_Store'):  # Exclude .DS_Store files\n",
        "                            self.samples.append((img_path, class_label))\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_label = self.samples[idx]\n",
        "\n",
        "        # Load the image as a PIL Image\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        # Convert to a 3-channel image by duplicating the single channel\n",
        "        image = np.array(image)  # Convert PIL Image to NumPy array\n",
        "        image = np.stack([image] * 3, axis=-1)  # Duplicate the single channel to create three channels\n",
        "\n",
        "        # Convert NumPy array back to PIL Image\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, class_label\n",
        "\n",
        "# Define the path to your chest X-ray dataset\n",
        "DATA_PATH = \"/content/drive/MyDrive/HKU/Projects/GAN/chest_xray/chest_xray/train\"\n",
        "X_DIM = (64,64)  # You can adjust this to your desired image size\n",
        "BATCH_SIZE = 128  # You can adjust this batch size as needed\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(X_DIM),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Create the ImageDataset for the 'normal' class\n",
        "dataset_ood = OODImageDataset(root=DATA_PATH, transform=transform, target_class='LATERAL')\n",
        "dataloader_ood = torch.utils.data.DataLoader(dataset_ood, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68jDebHvzdGB"
      },
      "outputs": [],
      "source": [
        "#@title Frontal Data\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transform=None, target_transform=None, target_class='Frontal'):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.samples = []  # List to store sample paths and labels\n",
        "\n",
        "        # Ensure that the target_class is valid\n",
        "        if target_class not in ['NORMAL', 'PNEUMONIA', 'LATERAL']:\n",
        "            raise ValueError(\"target_class must be either 'NORMAL' or 'PNEUMONIA' or 'LATERAL'.\")\n",
        "\n",
        "        # Loop through the subdirectories (classes) in the dataset root\n",
        "        for class_name in os.listdir(root):\n",
        "            if class_name == target_class:\n",
        "                class_dir = os.path.join(root, class_name)\n",
        "                if os.path.isdir(class_dir):\n",
        "                    class_label = class_name  # You can use class_to_idx.get(class_name, class_name) for labels\n",
        "                    for img_name in os.listdir(class_dir):\n",
        "                        img_path = os.path.join(class_dir, img_name)\n",
        "                        if not img_name.startswith('.DS_Store'):  # Exclude .DS_Store files\n",
        "                            self.samples.append((img_path, class_label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_label = self.samples[idx]\n",
        "\n",
        "        # Load the image as a PIL Image\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        # Convert to a 3-channel image by duplicating the single channel\n",
        "        image = np.array(image)  # Convert PIL Image to NumPy array\n",
        "        image = np.stack([image] * 3, axis=-1)  # Duplicate the single channel to create three channels\n",
        "\n",
        "        # Convert NumPy array back to PIL Image\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, class_label\n",
        "\n",
        "# Define the path to your chest X-ray dataset\n",
        "DATA_PATH = \"/content/drive/MyDrive/HKU/Projects/GAN/chest_xray/chest_xray/train\"\n",
        "X_DIM = (64, 64)  # You can adjust this to your desired image size\n",
        "BATCH_SIZE = 128  # You can adjust this batch size as needed\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(X_DIM),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Create the ImageDataset for the 'normal' class\n",
        "dataset = ImageDataset(root=DATA_PATH, transform=transform, target_class='NORMAL')\n",
        "\n",
        "# Dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQagGFLWZF9F",
        "outputId": "2f8de9d6-1e7a-4ade-ad15-c0ae96e73017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#@title GAN\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input layer\n",
        "            nn.ConvTranspose2d(Z_DIM, G_HIDDEN * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(G_HIDDEN * 8),\n",
        "            nn.ReLU(True),\n",
        "            # 1st hidden layer\n",
        "            nn.ConvTranspose2d(G_HIDDEN * 8, G_HIDDEN * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(G_HIDDEN * 4),\n",
        "            nn.ReLU(True),\n",
        "            # 2nd hidden layer\n",
        "            nn.ConvTranspose2d(G_HIDDEN * 4, G_HIDDEN * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(G_HIDDEN * 2),\n",
        "            nn.ReLU(True),\n",
        "            # 3rd hidden layer\n",
        "            nn.ConvTranspose2d(G_HIDDEN * 2, G_HIDDEN, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(G_HIDDEN),\n",
        "            nn.ReLU(True),\n",
        "            # output layer\n",
        "            nn.ConvTranspose2d(G_HIDDEN, IMAGE_CHANNEL, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # 1st layer\n",
        "            nn.Conv2d(IMAGE_CHANNEL, D_HIDDEN, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 2nd layer\n",
        "            nn.Conv2d(D_HIDDEN, D_HIDDEN * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(D_HIDDEN * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 3rd layer\n",
        "            nn.Conv2d(D_HIDDEN * 2, D_HIDDEN * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(D_HIDDEN * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 4th layer\n",
        "            nn.Conv2d(D_HIDDEN * 4, D_HIDDEN * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(D_HIDDEN * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # output layer\n",
        "            nn.Conv2d(D_HIDDEN * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, 1).squeeze(1)\n",
        "\n",
        "\n",
        "# Create the generator\n",
        "netG = Generator().to(device)\n",
        "netG.apply(weights_init)\n",
        "print(netG)\n",
        "\n",
        "# Create the discriminator\n",
        "netD = Discriminator().to(device)\n",
        "netD.apply(weights_init)\n",
        "print(netD)\n",
        "\n",
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that I will use to visualize the progression of the generator\n",
        "viz_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1, device=device)\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVlOeAgO4B8s",
        "outputId": "4d452fb1-0e69-4779-ce65-6687dd971a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image 0, Step 0, Loss: 0.5773603916168213, KS Statistic: 0.4996010578600262, P-value: 1.328747138199462e-23\n",
            "Image 0, Step 100, Loss: 0.3942347764968872, KS Statistic: 0.45909284246226184, P-value: 8.466900970210012e-20\n",
            "Image 0, Step 200, Loss: 0.3719863295555115, KS Statistic: 0.432573108206318, P-value: 1.5931896129706993e-17\n",
            "Image 0, Step 300, Loss: 0.3567718267440796, KS Statistic: 0.3969975850984505, P-value: 9.988325698726413e-15\n",
            "Image 0, Step 400, Loss: 0.3419489860534668, KS Statistic: 0.3818965008209943, P-value: 1.2649613298633668e-13\n",
            "Image 0, Step 500, Loss: 0.32572853565216064, KS Statistic: 0.35407996852742923, P-value: 1.0121753728595365e-11\n",
            "Image 0, Step 600, Loss: 0.31259244680404663, KS Statistic: 0.3359714394568195, P-value: 1.43654744002546e-10\n",
            "Image 0, Step 700, Loss: 0.2906751334667206, KS Statistic: 0.31367909456678555, P-value: 3.0462579775869196e-09\n",
            "Image 0, Step 800, Loss: 0.2276442050933838, KS Statistic: 0.285031466777156, P-value: 1.10408669653609e-07\n",
            "Image 0, Step 900, Loss: 0.17952904105186462, KS Statistic: 0.2781761518926458, P-value: 2.467788250900673e-07\n",
            "Image 0, Step 1000, Loss: 0.1652470827102661, KS Statistic: 0.26819175476177376, P-value: 7.670268245853826e-07\n",
            "Image 0, Step 1100, Loss: 0.14674417674541473, KS Statistic: 0.2430956969752739, P-value: 1.0930122871835669e-05\n"
          ]
        }
      ],
      "source": [
        "#@title Optimization\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import os\n",
        "from scipy.stats import kstest\n",
        "import numpy as np\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/HKU/Projects/GAN/checkpoints\"\n",
        "checkpoint_file = \"checkpoint_epoch_999.pt\"\n",
        "checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "\n",
        "netG.load_state_dict(checkpoint['netG_state_dict'])\n",
        "optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
        "G_losses = checkpoint['G_losses']\n",
        "netG.to(device)\n",
        "netG.eval()  # Set the generator network to evaluation mode\n",
        "\n",
        "def optimize_z(netG, X, lr=0.001, loss_threshold=0.15, extra_steps_after_threshold=95):\n",
        "    z_vectors = []\n",
        "\n",
        "    for i in range(X.size(0)):  # Loop over each image in X\n",
        "        z = torch.zeros(1, Z_DIM, 1, 1, device=device, requires_grad=True)  # Initialize z\n",
        "        optimizer = optim.Adam([z], lr=lr)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        loss_item = float('inf')\n",
        "        step = 0\n",
        "        steps_after_threshold = 0\n",
        "\n",
        "        while steps_after_threshold < extra_steps_after_threshold:\n",
        "            optimizer.zero_grad()\n",
        "            G_z = netG(z)\n",
        "            loss = loss_fn(G_z, X[i].unsqueeze(0))  # Compare with the i-th image in X\n",
        "            loss_item = loss.item()\n",
        "            loss.backward()                         # gradients of the loss\n",
        "            optimizer.step()                        # Update the latent vector\n",
        "\n",
        "            # Perform KS test\n",
        "            z_flat = z.detach().cpu().numpy().flatten()  # Flatten and move z to CPU for KS test\n",
        "            ks_statistic, p_value = kstest(z_flat, 'norm')\n",
        "\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Image {i}, Step {step}, Loss: {loss_item}, KS Statistic: {ks_statistic}, P-value: {p_value}\")\n",
        "\n",
        "            step += 1\n",
        "\n",
        "\n",
        "            if loss_item <= loss_threshold:\n",
        "                steps_after_threshold += 1\n",
        "            else:\n",
        "                steps_after_threshold = 0\n",
        "\n",
        "        z_vectors.append(z.detach())\n",
        "\n",
        "    return torch.cat(z_vectors, dim=0)\n",
        "\n",
        "\n",
        "def load_image(image_path, common_size=(64, 64), image_channels=3):\n",
        "    image = imread(image_path)\n",
        "    image_resized = resize(image, common_size, anti_aliasing=True)\n",
        "\n",
        "    if image_resized.ndim == 2: # If the image is grayscale, convert to 3 channels\n",
        "        image_resized = np.stack((image_resized,)*3, axis=-1)\n",
        "\n",
        "    image_normalized = (image_resized - 0.5) / 0.5 # Normalize to [-1, 1]\n",
        "    image_tensor = torch.tensor(image_normalized.transpose((2, 0, 1)), dtype=torch.float32)\n",
        "\n",
        "    return image_tensor.unsqueeze(0) # Add a batch dimension\n",
        "\n",
        "# Load all images from the folder\n",
        "folder_path = \"/content/drive/MyDrive/HKU/Projects/GAN/test\"\n",
        "image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "if image_files:\n",
        "    X = []\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        X.append(load_image(image_path, common_size=(64, 64), image_channels=3))\n",
        "\n",
        "    X = torch.cat(X, dim=0).to(device)\n",
        "    optimized_z = optimize_z(netG, X)\n",
        "else:\n",
        "    print(\"No images found in the folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaId0LjOrMyh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}